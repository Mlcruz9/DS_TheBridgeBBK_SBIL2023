{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping con Beautiful Soup (lvl. 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como científico de datos, tarde o temprano llegarás a un punto en el que tendrás que recopilar grandes cantidades de datos. Ya sea un proyecto o por pasatiempo y no siempre podremos contar con las API, pero tenemos el web scraping... ¡Y una de las mejores herramientas de web scraping es Beautiful Soup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Pero qué es el web scraping?\n",
    "\n",
    "En pocas palabras, el web scraping es la recopilación automatizada de datos de sitios web (para ser más precisos, del contenido HTML de los sitios web).\n",
    "\n",
    "En este notebook, aprenderás los conceptos básicos sobre cómo extraer datos de HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la experiencia completa de Beautiful Soup, también deberás instalar un parswer, dentro de ellos tenemos..\n",
    "\n",
    "- html.parser\n",
    "- lxml\n",
    "- html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml\n",
    "#!pip install html.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0: Guardar la url del sitio en una variable\n",
    "sitio = \"https://rivaquiroga.github.io/taller-web-scraping-python-2023/ejercicio-1.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Hacer una \"solicitud\" a la página web para traer el código fuente\n",
    "respuesta = requests.get(sitio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sé si se guardo correctamente el sitio web?\n",
    "\n",
    "Posibles respuestas:\n",
    "\n",
    "- [Respuestas informativas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#information_responses) (100–199)\n",
    "- [Respuestas exitosas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#successful_responses) (200–299)\n",
    "- [Mensajes de redirección](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#redirection_messages) (300–399)\n",
    "- [Respuestas de error del cliente](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses) (400–499)\n",
    "- [Respuestas de error del servidor](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses) (500–599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat&display=swap\" rel=\"stylesheet\"> \n",
      "<link href=\"https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n",
      "<link rel=\"stylesheet\" href=\"estilos.css\">\n",
      "</head>\n",
      "<body>\n",
      "<h1>Librerías para hacer web scraping</h1>\n",
      "<p>\n",
      "\tEstas son algunas de las librerías más conocidas para hacer web scraping con Python:\n",
      "</p>\n",
      "<div>\n",
      "\t<h2>BeautifulSoup</h2>\n",
      "\t<p class=\"librerias\">Es fácil de instalar y práctica para personas que se instán iniciando en web scraping. Con unas pocas líneas de código podemos obtener los datos que nos interesan. Solo funciona para sitios web estáticos, es decir, no sirve para páginas que cargan datos de forma dinámica con JavaScript. </p>\n",
      "\t<a href=\"https://www.crummy.com/software/BeautifulSoup/\">Más info</a>\n",
      "\t<h2>Selenium</h2>\n",
      "\t<p class=\"librerias\">Es una herramienta diseñada originalmente para automatización y testeo de aplicaciones web (por lo tanto, su foco no es extraer datos). Selenium puede trabajar con páginas dinámicas que usan JavaScript y es más fácil de aprender que Scrapy. Es útil para proyectos pequeños en que la velocidad no sea una prioridad. A veces la instalación no es tan fácil. \n",
      "\t</p>\n",
      "\t<a href=\"https://www.selenium.dev/\">Más info</a>\n",
      "\t<h2>Scrapy</h2>\n",
      "\t<p class=\"librerias\">Es un framework creado para hacer web scraping. Es rápido y completo, pero es más difícil de aprender.</p>\n",
      "\t<a href=\"https://scrapy.org/\">Más info</a>\n",
      "</div>\n",
      "<div class=\"nota\">\n",
      "\t<p class=\"texto-nota\">\n",
      "\t\t¿Qué librería elegir? La elección siempre dependerá las características del proyecto, del tiempo y de los recursos computacionales disponibles.\n",
      "\t</p>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: \n",
    "contenido = respuesta.text\n",
    "\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el resultado obtenido en HTML de la página, pero es realmente difícil de leer... Pero para eso usamos *BeautifulSoup* y *html.parser*. \n",
    "Lo primero que necesitamos es crear un objeto BS, comúnmente llamado **soup**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat&amp;display=swap\" rel=\"stylesheet\"/>\n",
      "<link href=\"https://fonts.googleapis.com/css2?family=Source+Code+Pro&amp;display=swap\" rel=\"stylesheet\"/>\n",
      "<link href=\"estilos.css\" rel=\"stylesheet\"/>\n",
      "</head>\n",
      "<body>\n",
      "<h1>Librerías para hacer web scraping</h1>\n",
      "<p>\n",
      "\tEstas son algunas de las librerías más conocidas para hacer web scraping con Python:\n",
      "</p>\n",
      "<div>\n",
      "<h2>BeautifulSoup</h2>\n",
      "<p class=\"librerias\">Es fácil de instalar y práctica para personas que se instán iniciando en web scraping. Con unas pocas líneas de código podemos obtener los datos que nos interesan. Solo funciona para sitios web estáticos, es decir, no sirve para páginas que cargan datos de forma dinámica con JavaScript. </p>\n",
      "<a href=\"https://www.crummy.com/software/BeautifulSoup/\">Más info</a>\n",
      "<h2>Selenium</h2>\n",
      "<p class=\"librerias\">Es una herramienta diseñada originalmente para automatización y testeo de aplicaciones web (por lo tanto, su foco no es extraer datos). Selenium puede trabajar con páginas dinámicas que usan JavaScript y es más fácil de aprender que Scrapy. Es útil para proyectos pequeños en que la velocidad no sea una prioridad. A veces la instalación no es tan fácil. \n",
      "\t</p>\n",
      "<a href=\"https://www.selenium.dev/\">Más info</a>\n",
      "<h2>Scrapy</h2>\n",
      "<p class=\"librerias\">Es un framework creado para hacer web scraping. Es rápido y completo, pero es más difícil de aprender.</p>\n",
      "<a href=\"https://scrapy.org/\">Más info</a>\n",
      "</div>\n",
      "<div class=\"nota\">\n",
      "<p class=\"texto-nota\">\n",
      "\t\t¿Qué librería elegir? La elección siempre dependerá las características del proyecto, del tiempo y de los recursos computacionales disponibles.\n",
      "\t</p>\n",
      "</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: crear la \"sopa\"\n",
    "\n",
    "soup = BeautifulSoup(contenido, \"html.parser\")\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo navegar por un objeto de Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML consta de elementos como enlaces, párrafos, encabezados, bloques, etc. Estos elementos están envueltos entre etiquetas; dentro de la etiqueta de apertura y cierre se puede encontrar el contenido del elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\html-content-web-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los elementos HTML también pueden tener atributos que contienen información adicional sobre el elemento. Los atributos se definen en las etiquetas de apertura con la siguiente sintaxis: \n",
    "\n",
    "    *nombre del atributo* = \"valor del atributo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\attribute-example-for-web-scraping-1536x386.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Librerías para hacer web scraping</h1>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 4: buscar los datos que nos interesan dentro del código fuente de la página\n",
    "\n",
    "''' La función find() nos permite encontrar el primer elemento que tenga una determinada etiqueta/clase.\n",
    "Nos devuelve todo el elemento html (o sea, las etiquetas y el contenido) '''\n",
    "\n",
    "soup.find(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Librerías para hacer web scraping</h1>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En lugar de usar find(), también es posible llamar a las clases con .\n",
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Librerías para hacer web scraping'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para quedarnos solo con el texto, usamos el método get_text()\n",
    "soup.find(\"h1\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>BeautifulSoup</h2>, <h2>Selenium</h2>, <h2>Scrapy</h2>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si queremos encontrar todos los elementos que tengan una determinada etiqueta/clase, usamos el método find_all(). Eso nos devuelve una lista con todos esos elementos. \n",
    "soup.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BeautifulSoup'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No podemos utilizar get_text() con find_all() porque ese método funciona con cada elemento de forma individual, no con una lista:\n",
    "# Tenemos que aplicarlo a cada elemento de forma individual.\n",
    "soup.find_all(\"h2\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La opción más rápida sería iterar. Los pasos entonces, serían:\n",
    "\n",
    "# primero, guardar en  una variable la lista con todos los elementos h2:\n",
    "elementos_h2 = soup.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luego, iterar por los elementos de esa lista, aplicarles get_text() y guardar el texto en la lista librerías\n",
    "\n",
    "librerias = []\n",
    "for elemento in elementos_h2:\n",
    "    elemento = elemento.get_text()\n",
    "    librerias.append(elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BeautifulSoup', 'Selenium', 'Scrapy']\n"
     ]
    }
   ],
   "source": [
    "# ¡Listo!\n",
    "print(librerias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, extraigamos las descripciones:\n",
    "# Como hay varias cosas etiquetadas como \"p\", tenemos que especificar la clase para que nos devuelva solo las que nos interesan\n",
    "\n",
    "elementos_p = soup.find_all(\"p\", class_ = \"librerias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tener en cuenta\n",
    "\n",
    "1. Con **soup.find_all('p')** encontramos cada elemento *párrafo* en la página web.\n",
    "\n",
    "2. Con **class_= 'librerias'** especificamos que buscamos específicamente etiquetas p que contengan el atributo **class_= 'librerias'** \n",
    "\n",
    "                Nota importante: el “_”  en    class**__**=”title”  no es un error tipográfico, se requiere en Beautiful Soup cuando seleccionando atributos de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[<p class=\"librerias\">Es fácil de instalar y práctica para personas que se instán iniciando en web scraping. Con unas pocas líneas de código podemos obtener los datos que nos interesan. Solo funciona para sitios web estáticos, es decir, no sirve para páginas que cargan datos de forma dinámica con JavaScript. </p>, <p class=\"librerias\">Es una herramienta diseñada originalmente para automatización y testeo de aplicaciones web (por lo tanto, su foco no es extraer datos). Selenium puede trabajar con páginas dinámicas que usan JavaScript y es más fácil de aprender que Scrapy. Es útil para proyectos pequeños en que la velocidad no sea una prioridad. A veces la instalación no es tan fácil. \n",
      "\t</p>, <p class=\"librerias\">Es un framework creado para hacer web scraping. Es rápido y completo, pero es más difícil de aprender.</p>]\n"
     ]
    }
   ],
   "source": [
    "# siempre es útil chequear con len() si la cantidad de elementos es la que esperábamos\n",
    "print(len(elementos_p))\n",
    "print(elementos_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Es fácil de instalar y práctica para personas que se instán iniciando en web scraping. Con unas pocas líneas de código podemos obtener los datos que nos interesan. Solo funciona para sitios web estáticos, es decir, no sirve para páginas que cargan datos de forma dinámica con JavaScript.', 'Es una herramienta diseñada originalmente para automatización y testeo de aplicaciones web (por lo tanto, su foco no es extraer datos). Selenium puede trabajar con páginas dinámicas que usan JavaScript y es más fácil de aprender que Scrapy. Es útil para proyectos pequeños en que la velocidad no sea una prioridad. A veces la instalación no es tan fácil.', 'Es un framework creado para hacer web scraping. Es rápido y completo, pero es más difícil de aprender.']\n"
     ]
    }
   ],
   "source": [
    "# Y ahora iteramos. En este caso agregamos \"strip = True\" a get_text(). \n",
    "'''Con esta opción podemos eliminar espacios antes y después del texto que nos interesa.'''\n",
    "\n",
    "descripciones = []\n",
    "for elemento in elementos_p:\n",
    "    elemento = elemento.get_text(strip = True)\n",
    "    descripciones.append(elemento)\n",
    "\n",
    "print(descripciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Más info'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente, extraemos los enlaces.\n",
    "# En este caso no nos sirve get_text(), porque nos devuelve el texto al que está asociado el enlace, no el enlace propiamente tal\n",
    "soup.find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.crummy.com/software/BeautifulSoup/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para obtener el enlace usamos get() e indicamos que queremos el \"href\". Esto es igual para todos los sitios web.\n",
    "soup.find(\"a\").get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora, repetimos el proceso: guardamos todo en una lista y luego iteramos para guardar solo los enlaces en una lista nueva.\n",
    "\n",
    "elementos_a = soup.find_all(\"a\")\n",
    "len(elementos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.crummy.com/software/BeautifulSoup/', 'https://www.selenium.dev/', 'https://scrapy.org/']\n"
     ]
    }
   ],
   "source": [
    "enlaces = []\n",
    "for elemento in elementos_a:\n",
    "    elemento = elemento.get(\"href\")\n",
    "    enlaces.append(elemento)\n",
    "\n",
    "print(enlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'libreria': ['BeautifulSoup', 'Selenium', 'Scrapy'], 'descripcion': ['Es fácil de instalar y práctica para personas que se instán iniciando en web scraping. Con unas pocas líneas de código podemos obtener los datos que nos interesan. Solo funciona para sitios web estáticos, es decir, no sirve para páginas que cargan datos de forma dinámica con JavaScript.', 'Es una herramienta diseñada originalmente para automatización y testeo de aplicaciones web (por lo tanto, su foco no es extraer datos). Selenium puede trabajar con páginas dinámicas que usan JavaScript y es más fácil de aprender que Scrapy. Es útil para proyectos pequeños en que la velocidad no sea una prioridad. A veces la instalación no es tan fácil.', 'Es un framework creado para hacer web scraping. Es rápido y completo, pero es más difícil de aprender.'], 'enlace': ['https://www.crummy.com/software/BeautifulSoup/', 'https://www.selenium.dev/', 'https://scrapy.org/']}\n"
     ]
    }
   ],
   "source": [
    "# Guardar todo en un data frame\n",
    "\n",
    "web_scraping = {\"libreria\": librerias, \"descripcion\": descripciones, \"enlace\": enlaces}\n",
    "print(web_scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>libreria</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>enlace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BeautifulSoup</td>\n",
       "      <td>Es fácil de instalar y práctica para personas ...</td>\n",
       "      <td>https://www.crummy.com/software/BeautifulSoup/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selenium</td>\n",
       "      <td>Es una herramienta diseñada originalmente para...</td>\n",
       "      <td>https://www.selenium.dev/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scrapy</td>\n",
       "      <td>Es un framework creado para hacer web scraping...</td>\n",
       "      <td>https://scrapy.org/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        libreria                                        descripcion  \\\n",
       "0  BeautifulSoup  Es fácil de instalar y práctica para personas ...   \n",
       "1       Selenium  Es una herramienta diseñada originalmente para...   \n",
       "2         Scrapy  Es un framework creado para hacer web scraping...   \n",
       "\n",
       "                                           enlace  \n",
       "0  https://www.crummy.com/software/BeautifulSoup/  \n",
       "1                       https://www.selenium.dev/  \n",
       "2                             https://scrapy.org/  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_librerias = pd.DataFrame(web_scraping)\n",
    "df_librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   libreria     3 non-null      object\n",
      " 1   descripcion  3 non-null      object\n",
      " 2   enlace       3 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 204.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_librerias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el data frame\n",
    "\n",
    "df_librerias.to_csv(\"datos-extraidos/librerias-web-scraping.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
